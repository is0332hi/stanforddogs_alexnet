# -*- coding: utf-8 -*-
"""stanforddogs_alexnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18BIrAqY3imAonw7ViM0uBWwtNOevoTqa
"""

import numpy as np
from google.colab import drive
drive.mount("./gdrive")
drive_root_dir = "./gdrive/My Drive/"

# X = images, Z = labels
X = []
Z = []
imgsize = 227
N = 10

from tqdm import tqdm
import os
import cv2

def label_assignment(img,label):
    return label

#tqdm == progress var
def training_data(label,data_dir):
    for img in tqdm(os.listdir(data_dir)):
        label = label_assignment(img,label)
        path = os.path.join(data_dir,img)
        img = cv2.imread(path,cv2.IMREAD_COLOR)
        img = cv2.resize(img,(imgsize,imgsize))
        
        X.append(np.array(img))
        Z.append(str(label))

chihuahua_dir = drive_root_dir + 'Images/n02085620-Chihuahua'
japanese_spaniel_dir = drive_root_dir + 'Images/n02085782-Japanese_spaniel'
maltese_dir = drive_root_dir + 'Images/n02085936-Maltese_dog'
pekinese_dir = drive_root_dir + 'Images/n02086079-Pekinese'
shitzu_dir = drive_root_dir + 'Images/n02086240-Shih-Tzu'
blenheim_spaniel_dir = drive_root_dir + 'Images/n02086646-Blenheim_spaniel'
papillon_dir = drive_root_dir + 'Images/n02086910-papillon'
toy_terrier_dir = drive_root_dir + 'Images/n02087046-toy_terrier'
afghan_hound_dir = drive_root_dir + 'Images/n02088094-Afghan_hound'
basset_dir = drive_root_dir + 'Images/n02088238-basset'

training_data('chihuahua',chihuahua_dir)
training_data('japanese_spaniel',japanese_spaniel_dir)
training_data('maltese',maltese_dir)
training_data('pekinese',pekinese_dir)
training_data('shitzu',shitzu_dir)
training_data('blenheim_spaniel',blenheim_spaniel_dir)
training_data('papillon',papillon_dir)
training_data('toy_terrier',toy_terrier_dir)
training_data('afghan_hound',afghan_hound_dir)
training_data('basset',basset_dir)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical

label_encoder= LabelEncoder()
Y = label_encoder.fit_transform(Z)
Y = to_categorical(Y,10)
X = np.array(X)
X = X/255

#random_state == seed
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=69)
x_train1, x_valid, y_train1, y_valid = train_test_split(x_train, y_train, test_size=0.175)

from keras.preprocessing.image import ImageDataGenerator
augs_gen = ImageDataGenerator(
        featurewise_center=False,  
        samplewise_center=False, 
        featurewise_std_normalization=False,  
        samplewise_std_normalization=False,  
        zca_whitening=False,  
        rotation_range=10,  
        zoom_range = 0.1, 
        width_shift_range=0.2,  
        height_shift_range=0.2, 
        horizontal_flip=True,  
        vertical_flip=False) 

augs_gen.fit(x_train)

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D

def Model():

    w = [64,128,256,256,192,512,512,N]
    #w = [96,256,384,384,256,4096,4096,N]
    kernel = [11,3,5,3,3,3,3,3]

    model = Sequential()
    model.add(Conv2D(w[0], kernel_size=(kernel[0],kernel[0]), strides = 4, activation='relu', input_shape=(227, 227, 3)))
    model.add(MaxPooling2D(pool_size=(kernel[1],kernel[1]),  strides = 2, padding='same'))
    model.add(Conv2D(w[1], kernel_size=(kernel[2],kernel[2]), activation='relu'))
    model.add(MaxPooling2D(pool_size=(kernel[3],kernel[3]),  strides = 2))
    model.add(Conv2D(w[2], kernel_size=(kernel[4],kernel[4]), activation='relu'))
    model.add(Conv2D(w[3], kernel_size=(kernel[5],kernel[5]), activation='relu'))
    model.add(Conv2D(w[4], kernel_size=(kernel[6],kernel[6]), activation='relu'))
    model.add(MaxPooling2D(pool_size=(kernel[7],kernel[7]), strides = 2))
    model.add(Flatten())
    model.add(Dense(w[5]))
    model.add(Dropout(0.5))
    model.add(Dense(w[6]))
    model.add(Dropout(0.5))
    model.add(Dense(w[7],activation='softmax'))

    return model

model = Model()
model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='SGD',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    batch_size=8,
                    epochs=50,
                    verbose=1,
                    validation_data=(x_valid, y_valid))

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

from matplotlib import pyplot as plt
# 精度のplot
plt.plot(history.history['acc'], marker='.', label='acc')
plt.plot(history.history['val_acc'], marker='.', label='val_acc')
plt.title('model accuracy')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(loc='best')
plt.show()

# 損失のplot
plt.plot(history.history['loss'], marker='.', label='loss')
plt.plot(history.history['val_loss'], marker='.', label='val_loss')
plt.title('model loss')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(loc='best')
plt.show()

#model.save(drive_root_dir + "alexdogs_model.h5")

from sklearn.metrics import confusion_matrix
predict_classes = model.predict_classes(x_test)
true_classes = np.argmax(y_test,1)
print(confusion_matrix(true_classes,predict_classes))